{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrDyhtdcFJ_d",
        "outputId": "48aef82f-1ac0-4d96-c951-766d31bf5d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torchtext==0.8.0 --quiet\n",
        "!pip install torch==1.7.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb-cNA1mXU-W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import random\n",
        "import functools\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "dir = 'drive/MyDrive/BS/DATA_EXTRACTION/'\n",
        "corp_cased = dir + 'corp_cased.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rhjHf7IJH-_",
        "outputId": "686ae090-a437-4f1e-de61-6a0d7f32f88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkjeMGfgXU-b"
      },
      "source": [
        "Установка seed для воспроизводимости результатов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv-PYbIUXU-b"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTdEdWGuteSg"
      },
      "outputs": [],
      "source": [
        "MODE_RU = True #True - rus, False - multilanguage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP82vIXvE4au"
      },
      "source": [
        "# Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj4DBdJ4XU-e"
      },
      "source": [
        "Импортируем токенизатор для BERT. Он определяет как текст в модели должен обрабатываться. Также он содержит словарь предобученной модели BERT. К сожалению, для русского языка нет отдельной предобученной модели и токенизатор, поэтому будем использовать многоязычную \\\n",
        "`bert-base-multilingual-uncased`.\n",
        "\n",
        "Чтобы использовать предобученную модель нужно, чтобы словарь в точности совпадал со словарем предобученной модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS2XMkfxXU-f"
      },
      "outputs": [],
      "source": [
        "if MODE_RU:\n",
        "    tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "else:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9lRko1PXU-i"
      },
      "source": [
        "Нужно убедиться, что входная последовательность форматирована так же, как и предусмотрено в BERT\n",
        "\n",
        "BERT была обучена на последовательностях, начинающихся с токена `[CLS]`\n",
        "\n",
        "Пример преобразования:\n",
        "\n",
        "```python\n",
        "text = ['Я', 'пошел', 'в', 'тот', 'магазин']\n",
        "```\n",
        "\n",
        "```python\n",
        "text = ['[CLS]', 'Я', 'пошел', 'в', 'тот', 'магазин']\n",
        "```\n",
        "\n",
        "Необходимо дополнительно убедиться, что неизвестный токен обозначается как `[UNK]`, а также токен выравнивания длины (padding token) обозначается как `[PAD]`\n",
        "\n",
        "Получим специальные токены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ7kwurvXU-j",
        "outputId": "4793c3ea-3f67-45c7-9ddc-b613a67a3d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] [PAD] [UNK]\n"
          ]
        }
      ],
      "source": [
        "init_token = tokenizer.cls_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, pad_token, unk_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPSTsF38E4ay"
      },
      "source": [
        "Получим индексы специальных токенов с помощью `convert_tokens_to_ids`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4TdEVVWXU-n",
        "outputId": "0f35cf58-48c4-496f-dac0-f9b8220e8eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 0 100\n"
          ]
        }
      ],
      "source": [
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, pad_token_idx, unk_token_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0XDkA4aXU-r"
      },
      "source": [
        "Необходимо, чтобы длина предложений во входных последовательностях не превосходила максимальной длины последовательностей в предобученной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrAoyoYCXU-r",
        "outputId": "9f944f6d-74af-4a6c-c3e7-3e44c940fcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-multilingual-cased']\n",
        "\n",
        "print(max_input_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD_ChHl0XU-v"
      },
      "source": [
        "Определим две вспомогательные функции для словарей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F6mhvYQE4a1"
      },
      "source": [
        "Первая будет 'подрезать' входные последовательности токенов до максимальной желаемой длины. Далее токены будут преобразовываться в индексы с помощью словаря. Она будет использоваться для последовательностей, для которых мы хотим найти теги.\n",
        "\n",
        "Важно отметить, что послеодовательности будут подрезаться до max_length-1 из-за начального токена `[CLS]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Xjhu3jXU-v"
      },
      "outputs": [],
      "source": [
        "def cut_and_convert_to_id(tokens, tokenizer, max_input_length):\n",
        "    tokens = tokens[:max_input_length-1]\n",
        "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60tYSQuSE4a3"
      },
      "source": [
        "Вторая вспомогательная функция просто подрезает последователньости до максимальной желаемой длины. Это используется для тегов. Мы не хотим передавать теги через словарь предобученной модели так как мы преимущественно используем русский язык. Будем строить словарь сами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TumdN5LnXU-0"
      },
      "outputs": [],
      "source": [
        "def cut_to_max_length(tokens, max_input_length):\n",
        "    tokens = tokens[:max_input_length-1]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ts-F9d6E4a4"
      },
      "source": [
        "Мы используем библиотеку `functools`, чтобы передать в функцию аргументы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dgaT_fjXU-3"
      },
      "outputs": [],
      "source": [
        "text_preprocessor = functools.partial(cut_and_convert_to_id,\n",
        "                                      tokenizer = tokenizer,\n",
        "                                      max_input_length = 64)\n",
        "\n",
        "tag_preprocessor = functools.partial(cut_to_max_length,\n",
        "                                     max_input_length = 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8KzzDLvE4a5"
      },
      "source": [
        "Дальше определим `Field`\n",
        "\n",
        "Для поля `TEXT`, который будет обрабатывать последовательности, для которых нужно найти теги сделаем следующее:\\\n",
        "1) Не будем использовать словарь\\\n",
        "2) Приведем текст в нижний регистр, так как модель в нижнем регистре\\\n",
        "3) Предобработка будет проводиться функцией text_preprocessor\\\n",
        "4) Зададим индексы специальных токенов\\\n",
        "\n",
        "Для поля `UD_TAGS` нужно убедиться, что длина последовательности тегов соответствует длине текстовой последовательности. Так как мы добавляли `[CLS]` токен в начало текстовых последовательностей, то нужно проделать то же самое и с тегами. Добавим `<pad>` токен в начало и укажем модели не использовать его при подсчете метрик качества. У нас также не будет неизвестных тегов. Функция предобработки - `tag_preprocessor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhJEvvF3XU-6"
      },
      "outputs": [],
      "source": [
        "TEXT = Field(use_vocab = False,\n",
        "                  lower = False,\n",
        "                  preprocessing = text_preprocessor,\n",
        "                  init_token = init_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "UD_TAGS = Field(unk_token = '<unk>',\n",
        "                     init_token = '<pad>',\n",
        "                     preprocessing = tag_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcKQVGNRE4a6"
      },
      "source": [
        "Определим какие из созданны полей соотвествуют полям в датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrTXFdSgXU--"
      },
      "outputs": [],
      "source": [
        "fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJotJpLIXU_B"
      },
      "source": [
        "Загрузим датасет и сразу разделим его"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn7OmDceXU_C"
      },
      "outputs": [],
      "source": [
        "dataset = TabularDataset(path=corp_cased, format='tsv', fields=fields, skip_header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2sLZ4CoE4a7"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = dataset.split([0.8, 0.1, 0.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPhIecHJXU_E"
      },
      "source": [
        "Пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klpqOEHJXU_I",
        "outputId": "c4264de2-c72a-4c69-8099-61d3eba6c7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': [5453, 66814, 16781, 42755, 9620, 47602, 27409, 30694, 23530, 851, 24265, 61690, 24145, 12908], 'udtags': ['VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'INFN', 'NOUN', 'PRTF', 'NOUN', 'CONJ', 'INFN', 'NOUN', 'ADJF', 'NOUN']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3HiRqq8XU_H"
      },
      "source": [
        "Нужно построить словарь тегов. Сделаем это с помощью `.build_vocab` на `train_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLyQaXWCE4a8",
        "outputId": "ef753a43-364d-4486-cf94-f3cf7ad3b391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f1953b3bf90>>, {'<unk>': 0, '<pad>': 1, 'NOUN': 2, 'ADJF': 3, 'PREP': 4, 'VERB': 5, 'CONJ': 6, 'ADVB': 7, 'NPRO': 8, 'PRCL': 9, 'INFN': 10, 'UNKN': 11, 'PRTF': 12, 'Geox': 13, 'Name': 14, 'Surn': 15, 'ADJS': 16, 'PRTS': 17, 'NUMR': 18, 'GRND': 19, 'PRED': 20, 'COMP': 21, 'INTJ': 22, 'Patr': 23})\n"
          ]
        }
      ],
      "source": [
        "UD_TAGS.build_vocab(train_data)\n",
        "\n",
        "print(UD_TAGS.vocab.stoi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtryK9EkE4a9"
      },
      "source": [
        "Определим итераторы. Они задают к порции данных будут подаваться во время обучения. Определим размер порции и также определим `device`, который автоматически будет подавать порции на GPU\n",
        "\n",
        "BERT достаточно большая модель, поэтому размер порции сравнительно небольшой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgIEyEkpXU_M"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    sort=False,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwJioco5E4a9"
      },
      "source": [
        "# Построение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rkAb2r3E4a-"
      },
      "source": [
        "Модель довольно проста, все сложные моменты скрыты внутри BERT. Можем считать, что BERT - это такой эмбеддинг слой и все что нам нужно сделать - это добавить линейный слой\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-pos-tagging/blob/master/assets/pos-bert.png?raw=1)\n",
        "\n",
        "В прошлом, желтые квадратики были эмбеддингами, которые создал соответствующий слой, но сейчас это эмбеддинги, которые мы получаем от предобученной модели BERT. Все входные значения подаются в BERT в одно и то же время. Стрелки между эмбеддингами BERT указывают, что эмбеддинги не считаются на для каждого токена индивидуально, а основываются на других токенах последовательности, что создает контекст.\n",
        "\n",
        "Обратим внимание, что мы не задаем `embedding_dim` для нашей модели. Это размер выхода из предобученной модели и мы не можем менять его. Таким образом, мы просто получаем эту размерность из атрибута модели `hidden_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYCKE0xXU_P"
      },
      "outputs": [],
      "source": [
        "class BERTPoSTagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 output_dim, \n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "  \n",
        "        #text = [sent len, batch size]\n",
        "    \n",
        "        text = text.permute(1, 0)\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.dropout(self.bert(text)[0])\n",
        "        \n",
        "        #embedded = [batch size, seq len, emb dim]\n",
        "                \n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "                    \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        predictions = self.fc(self.dropout(embedded))\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        \n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZtythM1E4a-"
      },
      "source": [
        "Далее, загрузим предобученную модель - прежде мы загружали только токенизатор модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q473f0kAXU_S",
        "outputId": "bdfcb827-0d37-42e2-aed1-69d931a91a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "if MODE_RU:\n",
        "    bert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "else:\n",
        "    bert = BertModel.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5yWrtQJE4a_"
      },
      "source": [
        "# Обучение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka_71WKzE4a_"
      },
      "source": [
        "Определим параметры для dropout слоя"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS-roKk0XU_V"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTPoSTagger(bert,\n",
        "                      OUTPUT_DIM, \n",
        "                      DROPOUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9BMrPqjE4bA"
      },
      "source": [
        "Посчитаем количество обучаемых параметров. Включаются параметры линейного слоя и все параметры BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHh9fi8rXU_Y",
        "outputId": "3c5c0f97-a7ce-4796-a384-8b1b6cc760d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 177,871,896 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SjWzecLE4bA"
      },
      "source": [
        "Определим оптимизатор. Обычно, при использовании предобученной модели мы используем коэффициент скорости обучения меньше, чем обычно. Это делается потому что мы не хотим радикально менять параметры, так как это может вызвать забывание того, что она выучила. Этот феномен называется катастрофическое забывание (catastrophic forgetting)\n",
        "\n",
        "Выбрали 5е-5, так как это одно из значений, рекомендованных создателями модели. Могут быть и лучшие значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qIKZQ2EXU_b"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhMileLME4bB"
      },
      "source": [
        "Определим функцию потерь, убеждаясь, что игнорируем токены заполнения (pad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJUz954cXU_e"
      },
      "outputs": [],
      "source": [
        "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGNagpu6XU_i"
      },
      "source": [
        "Дальше, просто помещаемс модель на GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ur1XIJQXU_j"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCT8V6yDE4bC"
      },
      "source": [
        "Определим функцию, которая подсчитываем точность предсказанных тегов, игнорируя токены заполнения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5G_0OckXU_o"
      },
      "source": [
        "Определим `train` и `evaluate` функции для обучения и тестирования модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk9vFsMuXU_p"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    all_preds = []\n",
        "    all_tags = []\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        text = batch.text\n",
        "        tags = batch.udtags\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        predictions = model(text)\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "        \n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        all_preds.append(predictions.detach().cpu().numpy())\n",
        "        all_tags.append(tags.detach().cpu().numpy())\n",
        "        \n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "        \n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMz_0UzGXU_s"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_tags = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.udtags\n",
        "            \n",
        "            predictions = model(text)\n",
        "            \n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            all_preds.append(predictions.detach().cpu().numpy())\n",
        "            all_tags.append(tags.detach().cpu().numpy())\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVIC3RFOE4bE"
      },
      "source": [
        "Определим вспомогаетльную функцию для подсчета времени"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57McNtpWXU_v"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cr_labels = list(range(2, len(UD_TAGS.vocab.itos)))\n",
        "cr_names = [UD_TAGS.vocab.itos[l] for l in cr_labels]\n",
        "\n",
        "print(cr_labels)\n",
        "print(cr_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkZhOpsh2tdI",
        "outputId": "b34c0968-4cf0-4614-80bc-24b789828fad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "['NOUN', 'ADJF', 'PREP', 'VERB', 'CONJ', 'ADVB', 'NPRO', 'PRCL', 'INFN', 'UNKN', 'PRTF', 'Geox', 'Name', 'Surn', 'ADJS', 'PRTS', 'NUMR', 'GRND', 'PRED', 'COMP', 'INTJ', 'Patr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEVwXxUKE4bE"
      },
      "source": [
        "Наконец, можем обучить модели.\n",
        "\n",
        "Эта модель обучается достаточно долго из-за большого количества параметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F53iNXOXU_y",
        "outputId": "cdee6de5-47ac-47af-dbcf-f6072e0c78f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 14m 39s\n",
            "\tTrain Loss: 0.303\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.11      0.96      0.20    405318\n",
            "        ADJF       0.86      0.92      0.89    185307\n",
            "        PREP       0.97      0.98      0.98    148372\n",
            "        VERB       0.81      0.92      0.86    110462\n",
            "        CONJ       0.78      0.97      0.87    109946\n",
            "        ADVB       0.44      0.87      0.59     46837\n",
            "        NPRO       0.96      0.90      0.93     37182\n",
            "        PRCL       0.87      0.92      0.89     37148\n",
            "        INFN       0.93      0.91      0.92     28957\n",
            "        UNKN       0.15      0.40      0.22     26973\n",
            "        PRTF       0.76      0.58      0.66     19312\n",
            "        Geox       0.83      0.84      0.84     17958\n",
            "        Name       0.68      0.73      0.70     14785\n",
            "        Surn       0.72      0.60      0.65     10881\n",
            "        ADJS       0.82      0.63      0.72      9953\n",
            "        PRTS       0.91      0.82      0.86      8518\n",
            "        NUMR       0.94      0.92      0.93      6401\n",
            "        GRND       0.81      0.52      0.64      5764\n",
            "        PRED       0.92      0.86      0.89      4094\n",
            "        COMP       0.95      0.74      0.83      2962\n",
            "        INTJ       0.80      0.67      0.73      2008\n",
            "        Patr       0.65      0.59      0.62       963\n",
            "\n",
            "   micro avg       0.26      0.91      0.40   1240101\n",
            "   macro avg       0.76      0.78      0.75   1240101\n",
            "weighted avg       0.59      0.91      0.64   1240101\n",
            "\n",
            "\t Val Loss: 0.213\n",
            "Epoch: 02 | Epoch Time: 14m 52s\n",
            "\tTrain Loss: 0.193\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.12      0.97      0.22    405318\n",
            "        ADJF       0.91      0.95      0.93    185307\n",
            "        PREP       0.97      1.00      0.98    148372\n",
            "        VERB       0.87      0.95      0.91    110462\n",
            "        CONJ       0.34      0.99      0.50    109946\n",
            "        ADVB       0.82      0.91      0.86     46837\n",
            "        NPRO       0.97      0.94      0.95     37182\n",
            "        PRCL       0.49      0.96      0.65     37148\n",
            "        INFN       0.95      0.94      0.95     28957\n",
            "        UNKN       0.22      0.51      0.30     26973\n",
            "        PRTF       0.84      0.69      0.76     19312\n",
            "        Geox       0.95      0.90      0.92     17958\n",
            "        Name       0.43      0.79      0.56     14785\n",
            "        Surn       0.79      0.71      0.75     10881\n",
            "        ADJS       0.86      0.77      0.81      9953\n",
            "        PRTS       0.95      0.89      0.92      8518\n",
            "        NUMR       0.96      0.97      0.96      6401\n",
            "        GRND       0.83      0.64      0.72      5764\n",
            "        PRED       0.95      0.93      0.94      4094\n",
            "        COMP       0.97      0.86      0.91      2962\n",
            "        INTJ       0.82      0.81      0.82      2008\n",
            "        Patr       0.90      0.78      0.83       963\n",
            "\n",
            "   micro avg       0.27      0.94      0.42   1240101\n",
            "   macro avg       0.77      0.86      0.78   1240101\n",
            "weighted avg       0.57      0.94      0.63   1240101\n",
            "\n",
            "\t Val Loss: 0.205\n",
            "Epoch: 03 | Epoch Time: 14m 52s\n",
            "\tTrain Loss: 0.155\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.14      0.97      0.24    405318\n",
            "        ADJF       0.93      0.96      0.94    185307\n",
            "        PREP       0.95      1.00      0.97    148372\n",
            "        VERB       0.92      0.96      0.94    110462\n",
            "        CONJ       0.16      0.99      0.28    109946\n",
            "        ADVB       0.64      0.92      0.75     46837\n",
            "        NPRO       0.97      0.95      0.96     37182\n",
            "        PRCL       0.33      0.96      0.49     37148\n",
            "        INFN       0.96      0.96      0.96     28957\n",
            "        UNKN       0.45      0.59      0.51     26973\n",
            "        PRTF       0.87      0.76      0.81     19312\n",
            "        Geox       0.96      0.92      0.94     17958\n",
            "        Name       0.56      0.82      0.67     14785\n",
            "        Surn       0.82      0.78      0.80     10881\n",
            "        ADJS       0.87      0.81      0.84      9953\n",
            "        PRTS       0.95      0.91      0.93      8518\n",
            "        NUMR       0.97      0.97      0.97      6401\n",
            "        GRND       0.85      0.72      0.78      5764\n",
            "        PRED       0.94      0.94      0.94      4094\n",
            "        COMP       0.97      0.88      0.92      2962\n",
            "        INTJ       0.83      0.83      0.83      2008\n",
            "        Patr       0.92      0.83      0.87       963\n",
            "\n",
            "   micro avg       0.27      0.95      0.42   1240101\n",
            "   macro avg       0.77      0.88      0.79   1240101\n",
            "weighted avg       0.56      0.95      0.62   1240101\n",
            "\n",
            "\t Val Loss: 0.214\n",
            "Epoch: 04 | Epoch Time: 14m 51s\n",
            "\tTrain Loss: 0.125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.22      0.98      0.36    405318\n",
            "        ADJF       0.94      0.96      0.95    185307\n",
            "        PREP       0.92      1.00      0.96    148372\n",
            "        VERB       0.93      0.97      0.95    110462\n",
            "        CONJ       0.08      0.99      0.15    109946\n",
            "        ADVB       0.52      0.93      0.67     46837\n",
            "        NPRO       0.72      0.95      0.82     37182\n",
            "        PRCL       0.08      0.97      0.16     37148\n",
            "        INFN       0.97      0.96      0.97     28957\n",
            "        UNKN       0.48      0.66      0.56     26973\n",
            "        PRTF       0.89      0.81      0.85     19312\n",
            "        Geox       0.97      0.94      0.95     17958\n",
            "        Name       0.63      0.86      0.73     14785\n",
            "        Surn       0.86      0.83      0.85     10881\n",
            "        ADJS       0.89      0.83      0.86      9953\n",
            "        PRTS       0.96      0.93      0.94      8518\n",
            "        NUMR       0.98      0.98      0.98      6401\n",
            "        GRND       0.88      0.80      0.83      5764\n",
            "        PRED       0.95      0.95      0.95      4094\n",
            "        COMP       0.97      0.90      0.94      2962\n",
            "        INTJ       0.84      0.83      0.83      2008\n",
            "        Patr       0.91      0.87      0.89       963\n",
            "\n",
            "   micro avg       0.28      0.96      0.43   1240101\n",
            "   macro avg       0.76      0.90      0.78   1240101\n",
            "weighted avg       0.56      0.96      0.63   1240101\n",
            "\n",
            "\t Val Loss: 0.228\n",
            "Epoch: 05 | Epoch Time: 14m 55s\n",
            "\tTrain Loss: 0.098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.29      0.98      0.45    405318\n",
            "        ADJF       0.96      0.97      0.97    185307\n",
            "        PREP       0.99      1.00      0.99    148372\n",
            "        VERB       0.95      0.97      0.96    110462\n",
            "        CONJ       0.06      0.99      0.11    109946\n",
            "        ADVB       0.82      0.95      0.88     46837\n",
            "        NPRO       0.98      0.96      0.97     37182\n",
            "        PRCL       0.12      0.97      0.21     37148\n",
            "        INFN       0.97      0.97      0.97     28957\n",
            "        UNKN       0.44      0.73      0.55     26973\n",
            "        PRTF       0.91      0.85      0.88     19312\n",
            "        Geox       0.98      0.96      0.97     17958\n",
            "        Name       0.11      0.88      0.20     14785\n",
            "        Surn       0.90      0.88      0.89     10881\n",
            "        ADJS       0.90      0.86      0.88      9953\n",
            "        PRTS       0.97      0.95      0.96      8518\n",
            "        NUMR       0.98      0.98      0.98      6401\n",
            "        GRND       0.90      0.86      0.88      5764\n",
            "        PRED       0.96      0.96      0.96      4094\n",
            "        COMP       0.98      0.92      0.94      2962\n",
            "        INTJ       0.83      0.85      0.84      2008\n",
            "        Patr       0.94      0.89      0.91       963\n",
            "\n",
            "   micro avg       0.28      0.97      0.43   1240101\n",
            "   macro avg       0.77      0.92      0.79   1240101\n",
            "weighted avg       0.61      0.97      0.68   1240101\n",
            "\n",
            "\t Val Loss: 0.256\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_preds, train_tags = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, _, __ = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "\n",
        "    print(classification_report(train_tags, train_preds, labels=cr_labels, target_names=cr_names))\n",
        "\n",
        "    print(f'\\t Val Loss: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cj8gvATE4bF"
      },
      "source": [
        "Можем загрузить лучшую модель и попробовать ее на тестовом множестве"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea6wyOs_XU_1",
        "outputId": "1f6800d0-380d-4aeb-fa80-4e5d9a11acd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.208\n",
            "[ 2  2  2 ... 11  2  2]\n",
            "[1 1 1 ... 2 1 1]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.11      0.96      0.20     50104\n",
            "        ADJF       0.91      0.94      0.93     23174\n",
            "        PREP       0.99      0.99      0.99     18333\n",
            "        VERB       0.91      0.94      0.93     13674\n",
            "        CONJ       0.76      0.99      0.86     13610\n",
            "        ADVB       0.94      0.90      0.92      5839\n",
            "        NPRO       0.97      0.94      0.96      4536\n",
            "        PRCL       0.96      0.96      0.96      4671\n",
            "        INFN       0.93      0.94      0.93      3738\n",
            "        UNKN       0.65      0.46      0.54      3401\n",
            "        PRTF       0.77      0.72      0.75      2442\n",
            "        Geox       0.94      0.91      0.92      2191\n",
            "        Name       0.84      0.80      0.82      1818\n",
            "        Surn       0.71      0.73      0.72      1310\n",
            "        ADJS       0.84      0.78      0.81      1178\n",
            "        PRTS       0.93      0.87      0.90      1029\n",
            "        NUMR       0.97      0.97      0.97       772\n",
            "        GRND       0.80      0.60      0.68       688\n",
            "        PRED       0.96      0.94      0.95       509\n",
            "        COMP       0.99      0.86      0.92       353\n",
            "        INTJ       0.84      0.82      0.83       278\n",
            "        Patr       0.82      0.79      0.81       120\n",
            "\n",
            "   micro avg       0.27      0.93      0.41    153768\n",
            "   macro avg       0.84      0.85      0.83    153768\n",
            "weighted avg       0.64      0.93      0.68    153768\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_preds, test_tags = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "print(test_preds[10:])\n",
        "print(test_tags[10:])\n",
        "\n",
        "print(classification_report(test_tags, test_preds, labels=cr_labels, target_names=cr_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpYFC9BNE4bF"
      },
      "source": [
        "# Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tzwSdf_E4bF"
      },
      "source": [
        "Мы увидим как использовать модель для тегирования последовательностей.\n",
        "\n",
        "Если мы подадим строку, то нужно ее разделить на индивидуальные токены. Мы сделаем это с помощью функции `tokenize` из `tokenizer`. После, перевести токены в числа так же, как мы делали раньше, используя `convert_tokens_to_ids`. Дальше добавляем `[CLS]` токен в начало последовательности\n",
        "\n",
        "Дальше, подаем последовательность в модель и получаем предсказания для каждого токена. Отсекаем `[CLS]`, так как нам он не интересен."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIOmo6FYXU_4"
      },
      "outputs": [],
      "source": [
        "def tag_sentence(model, device, sentence, tokenizer, text_field, tag_field):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(sentence, str):\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "    else:\n",
        "        tokens = sentence\n",
        "    \n",
        "    numericalized_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    numericalized_tokens = [text_field.init_token] + numericalized_tokens\n",
        "        \n",
        "    unk_idx = text_field.unk_token\n",
        "    \n",
        "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "    \n",
        "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "    \n",
        "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "         \n",
        "    predictions = model(token_tensor)\n",
        "    \n",
        "    top_predictions = predictions.argmax(-1)\n",
        "    \n",
        "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
        "    \n",
        "    predicted_tags = predicted_tags[1:]\n",
        "        \n",
        "    assert len(tokens) == len(predicted_tags)\n",
        "    \n",
        "    return tokens, predicted_tags, unks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJgXvSlFE4bG"
      },
      "source": [
        "Пример"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NAjXntVXU_7"
      },
      "outputs": [],
      "source": [
        "sentence = 'Павел Дуров сегодня анонсировал создание нового офиса в Санкт-Петербурге'\n",
        "\n",
        "tokens, tags, unks = tag_sentence(model, \n",
        "                                  device, \n",
        "                                  sentence,\n",
        "                                  tokenizer,\n",
        "                                  TEXT, \n",
        "                                  UD_TAGS)\n",
        "\n",
        "print(unks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHfrCjgIXU_-"
      },
      "outputs": [],
      "source": [
        "print(\"Pred. Tag\\tToken\\n\")\n",
        "\n",
        "for token, tag in zip(tokens, tags):\n",
        "    print(f\"{tag}\\t\\t{token}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KpYFC9BNE4bF"
      ],
      "machine_shape": "hm",
      "name": "src_tl_bert_ru_multi.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
