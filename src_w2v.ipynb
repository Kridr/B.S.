{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "src_w2v.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библоитеки"
      ],
      "metadata": {
        "id": "rzL9GPKra1Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from nltk.stem.snowball import RussianStemmer\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as data_utils\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "OWwej54saLSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Часть 1. Чтение данных и первичная предобработка"
      ],
      "metadata": {
        "id": "nLLLiXUQa5eA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В программе будет три режима работы:\n",
        "1.   Без уменьшения словаря\n",
        "2.   Стемминг\n",
        "3.   Лемматизация"
      ],
      "metadata": {
        "id": "Y9HsHUFMbFQQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJe5ClYi7Kr9"
      },
      "outputs": [],
      "source": [
        "REDUCTION_MODE = 0 #0 - no reduction, 1 - stemming, 2 - lemmatize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем гугл диск, где содержится файл с корпусом, который был получен ранее"
      ],
      "metadata": {
        "id": "QR3-c3Vebjzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "dir = 'drive/MyDrive/BS/DATA_EXTRACTION/'\n",
        "corp_cased = dir + 'corp_cased.csv'"
      ],
      "metadata": {
        "id": "WULuCwwH9LJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считываем csv файл корпуса и выводим первые пять элементов"
      ],
      "metadata": {
        "id": "3F03tQFjbznw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(corp_cased, sep='\\t', header=None, on_bad_lines='warn')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AOIpuBdx9TU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = RussianStemmer()\n",
        "lemmatizer = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "EOHrtjQdGzFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию, которая исходя из режима работы делает соотвестующие преобразования слов"
      ],
      "metadata": {
        "id": "L7kr_wTOb-7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduction(x):\n",
        "    x = str(x)\n",
        "    if REDUCTION_MODE == 0:\n",
        "        return x.split()\n",
        "    elif REDUCTION_MODE == 1:\n",
        "        return [stemmer.stem(token) for token in x.split(' ')]\n",
        "    elif REDUCTION_MODE == 2:\n",
        "        return [lemmatizer.normal_forms(token)[0] for token in x.split(' ')]"
      ],
      "metadata": {
        "id": "lzLqDhbz_TI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем функцию выше"
      ],
      "metadata": {
        "id": "jRPa8OUScGyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df[df.columns[0]].to_numpy()\n",
        "\n",
        "sentences = np.array(list(map(reduction, sentences)))\n",
        "\n",
        "print(sentences[:5])"
      ],
      "metadata": {
        "id": "H1teZPfiNQLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также разделяем теги на токены"
      ],
      "metadata": {
        "id": "eSsnmhUIcKOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = df[df.columns[1]].to_numpy()\n",
        "\n",
        "tags = np.array(list(map(lambda x: str(x).split(), tags)))\n",
        "\n",
        "print(tags[:5])"
      ],
      "metadata": {
        "id": "_rp79vyIXEGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем Word2Vec модель из имеющихся предложений"
      ],
      "metadata": {
        "id": "3cpEeNgUcolg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size, window, min_cnt, sg = 30, 2, 2, 0 # Используем модель CBOW\n",
        "workers = multiprocessing.cpu_count()\n",
        "n_iter = 150\n",
        "w2v_model = Word2Vec(sentences, size = size, window = window, min_count = min_cnt,\n",
        "                    sg = sg, workers = workers, iter = n_iter)"
      ],
      "metadata": {
        "id": "P8_ypRR3XFGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 2. Подготовка датасетов для моделей"
      ],
      "metadata": {
        "id": "Ty1kBtgHczZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем функции для создания словарей слов и тегов (чтобы перевести текст в цифру), а также создаем модель для классификатора\\\n",
        "[слово_до, слово, слово_после] -> часть речи"
      ],
      "metadata": {
        "id": "Hn9FkzRhc5eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_voc_w(stoi):\n",
        "    idx = 1\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in stoi:\n",
        "                stoi[word] = idx\n",
        "                idx += 1\n",
        "\n",
        "\n",
        "def build_voc_t(ttoi):\n",
        "    idx = 0\n",
        "    \n",
        "    for tags_ in tags:\n",
        "        for tag in tags_:\n",
        "            if tag not in ttoi:\n",
        "                ttoi[tag] = idx\n",
        "                idx += 1\n",
        "\n",
        "def creator(x, y, stoi, ttoi):\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences[i])):\n",
        "            x_elem = []\n",
        "            #word before\n",
        "            if j == 0:\n",
        "                x_elem.append(0)\n",
        "            else:\n",
        "                x_elem.append(stoi[sentences[i][j - 1]])\n",
        "\n",
        "            #current word\n",
        "            x_elem.append(stoi[sentences[i][j]])\n",
        "\n",
        "            #word after\n",
        "            if j == len(sentences[i]) - 1:\n",
        "                x_elem.append(0)\n",
        "            else:\n",
        "                x_elem.append(stoi[sentences[i][j + 1]])\n",
        "\n",
        "            x.append(x_elem)\n",
        "            y.append(ttoi[tags[i][j]])"
      ],
      "metadata": {
        "id": "b8I53BK-iICH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применяем определенные выше функции. Нулевой индекс оставляем для выравнивания"
      ],
      "metadata": {
        "id": "1K9kuWSmdUuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sentences vocs\n",
        "stoi = {None: 0}\n",
        "\n",
        "#tags vocs\n",
        "ttoi = {}\n",
        "\n",
        "build_voc_w(stoi)\n",
        "build_voc_t(ttoi)\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "creator(x, y, stoi, ttoi)"
      ],
      "metadata": {
        "id": "BTPKhFrviVUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример данных"
      ],
      "metadata": {
        "id": "c-kObblAdico"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:5])\n",
        "print(y[:5])"
      ],
      "metadata": {
        "id": "NvzEn9Czsmro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем устройство на котором будет обучаться модель"
      ],
      "metadata": {
        "id": "GMAj5VdRdlN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "owaWm1e7vFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переопределяем класс Dataset из torch"
      ],
      "metadata": {
        "id": "zjOBJjH3gW1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PosTagDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "        self.x = torch.LongTensor(self.x).to(device)\n",
        "        self.y = torch.LongTensor(self.y).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x[idx], self.y[idx])"
      ],
      "metadata": {
        "id": "9gFNFzvSQDlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делим данные на три множества: train, validation, test"
      ],
      "metadata": {
        "id": "E8b__Uvqgd5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, x_test = tuple(np.split(x, [int(.7 * len(x)), int(.8 * len(x))]))\n",
        "y_train, y_val, y_test = tuple(np.split(y, [int(.7 * len(y)), int(.8 * len(y))]))"
      ],
      "metadata": {
        "id": "VrMR9QCritJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем датасеты"
      ],
      "metadata": {
        "id": "TMa0BAUdgx9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = PosTagDataset(x_train, y_train)\n",
        "dataset_val = PosTagDataset(x_val, y_val)\n",
        "dataset_test = PosTagDataset(x_test, y_test)"
      ],
      "metadata": {
        "id": "tOAkG3wTb8EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем корректность метода '__getitem __'"
      ],
      "metadata": {
        "id": "0MxxpQhZhG3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_train.__getitem__(0))"
      ],
      "metadata": {
        "id": "zDCnpU0dtHFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем DataLoader на основе созданные ранее датасетов"
      ],
      "metadata": {
        "id": "ByFwdujyhpfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "PAskNf0JkRKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию для получения весов из W2V модели, чтобы в дальнейшем"
      ],
      "metadata": {
        "id": "HRHAaZK2hqhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#перевод word2vec в массив весов для слоя Embedding    \n",
        "def make_e_weights():\n",
        "    # wv.index2word - список слов словаря\n",
        "    # wv.vectors - массив координат слов\n",
        "    dict_w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.vectors))\n",
        "    e_weights = np.zeros((len(stoi), size))\n",
        "    for w, t in stoi.items(): # Слово и его код\n",
        "        w_coords = dict_w2v.get(w) # Координаты слова\n",
        "        if w_coords is not None:\n",
        "            e_weights[t] = w_coords\n",
        "    return torch.FloatTensor(e_weights)"
      ],
      "metadata": {
        "id": "1awTEQ1woAC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем модель"
      ],
      "metadata": {
        "id": "j8dGqlvviLyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class W2VPoSTagger(nn.Module):\n",
        "    def __init__(self, size_w2v, hidden_layer_s):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        weights = make_e_weights()\n",
        "        weights.to(device)\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=False)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(size_w2v * 3, hidden_layer_s)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_layer_s, len(ttoi))\n",
        "        self.act2 = nn.Softmax()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "GOaU-d1fqitQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем экземпляр модели"
      ],
      "metadata": {
        "id": "Ao9luUGziPpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = W2VPoSTagger(size_w2v=size, hidden_layer_s=30)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "XQiUmrJp553C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем количество параметров"
      ],
      "metadata": {
        "id": "TpIF_htIiWDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Модель имеет {count_parameters(model):,} обучаемых параметров')"
      ],
      "metadata": {
        "id": "0KK7dspH56RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 3. Обучение модели"
      ],
      "metadata": {
        "id": "iZr65H7pjaGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем оптимизатор и функцию потерь"
      ],
      "metadata": {
        "id": "SSLVhsRmi8C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-LLsSvZg6Hnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Помещаем модель и функцию потерь на `device`"
      ],
      "metadata": {
        "id": "FJVEm-2pjRYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "v9SAOm816LWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию обучения модели"
      ],
      "metadata": {
        "id": "JKu_n08ykDAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    all_preds = []\n",
        "    all_tags = []\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        text = batch[0]\n",
        "        tags = batch[1]\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(text)\n",
        "\n",
        "        all_preds.append(predictions.detach().cpu().numpy())\n",
        "        all_tags.append(tags.detach().cpu().numpy())\n",
        "\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)"
      ],
      "metadata": {
        "id": "GHUIs3L66PUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию валидации модели"
      ],
      "metadata": {
        "id": "nDa6MrdCkIHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_tags = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch[0]\n",
        "            tags = batch[1]\n",
        "            \n",
        "            predictions = model(text)\n",
        "\n",
        "            all_preds.append(predictions.detach().cpu().numpy())\n",
        "            all_tags.append(tags.detach().cpu().numpy())\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), np.concatenate(all_preds, 0).argmax(1).reshape(-1), np.concatenate(all_tags, 0)"
      ],
      "metadata": {
        "id": "Vm_3DZiX6TbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию для подсчета времени выполнения"
      ],
      "metadata": {
        "id": "yqToCQgWkMzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "snT_IcJe6W-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задаем параметры для `classification_report`"
      ],
      "metadata": {
        "id": "vhu1W_GEmEgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_labels = []\n",
        "cr_names = []\n",
        "\n",
        "for name, label in ttoi.items():\n",
        "    cr_labels.append(label)\n",
        "    cr_names.append(name)\n",
        "\n",
        "print(cr_labels)\n",
        "print(cr_names)"
      ],
      "metadata": {
        "id": "GasDGzLAZk_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем модель"
      ],
      "metadata": {
        "id": "xIdF6trwmLPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 15\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_preds, train_tags = train(model, dataloader_train, optimizer, criterion)\n",
        "    valid_loss, _, __ = evaluate(model, dataloader_val, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "\n",
        "    print(classification_report(train_tags, train_preds, labels=cr_labels, target_names=cr_names))\n",
        "\n",
        "    print(f'\\t Val Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "LvO_nXHl6fBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестируем модель на отложенной выборке"
      ],
      "metadata": {
        "id": "Ru-b_srcmoiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_preds, test_tags = evaluate(model, dataloader_test, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "print(test_preds[10:])\n",
        "print(test_tags[10:])\n",
        "\n",
        "print(classification_report(test_tags, test_preds, labels=cr_labels, target_names=cr_names))"
      ],
      "metadata": {
        "id": "1ZlL44226iiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Часть 4. Использование модели keras"
      ],
      "metadata": {
        "id": "vYlsiBEXoWRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем необходимые библиотеки"
      ],
      "metadata": {
        "id": "aUVFr34LofQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Embedding, Dropout, Reshape\n",
        "import time\n",
        "from sklearn import model_selection"
      ],
      "metadata": {
        "id": "BxAn3VhawXcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:10])\n",
        "print(y[:10])"
      ],
      "metadata": {
        "id": "pwbm-rp9zaCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем модель"
      ],
      "metadata": {
        "id": "jKVwKzFQp8II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#создание модели          \n",
        "def create_model(sq, num_words, size, num_classes):\n",
        "    inp = Input(shape = (sq, ), dtype = 'int32')\n",
        "    e_weights = make_e_weights()\n",
        "    x = Embedding(num_words, output_dim = size, input_length = sq, \n",
        "                weights = [e_weights], trainable = True)(inp)\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(size, activation = 'relu', use_bias = True)(x)\n",
        "    output = Dense(num_classes, activation = 'softmax', use_bias = True)(x)\n",
        "    model = Model(inp, output)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "AI7kIMuswi9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию-обертку для `clasification_report`"
      ],
      "metadata": {
        "id": "9eaB_-hHp-Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cl_rep(y_true, y_pred):\n",
        "    print(classification_report(y_true.numpy(), y_pred.numpy()))"
      ],
      "metadata": {
        "id": "aMLvR6OZxKRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем модель"
      ],
      "metadata": {
        "id": "m-OC6ykJqEhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#обучение НС\n",
        "model = create_model(3, len(stoi), 30, len(ttoi))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = losses.sparse_categorical_crossentropy, metrics = ['accuracy', cl_rep])\n",
        "\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.25)\n",
        "\n",
        "startTime = time.time()\n",
        "history = model.fit(x_train, y_train, batch_size = 128, epochs = 15, verbose = 2, validation_data = (x_test, y_test))\n",
        "print('Full time:', time.time() - startTime)"
      ],
      "metadata": {
        "id": "E6lGdb2Xwpxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}